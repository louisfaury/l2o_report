\chapter{Introduction}
	{
		\section{Criteo}
		{
		
			\paragraph{} Criteo is one of the most successful entrepreneurship adventures of the last few years, especially in the web technologies field. Funded in 2005 by Jean-Baptiste Rudelle, Franck le Ouay and Romain Niccoli, the company offers personalized ad-retargeting solutions. In only 3 years, the company becomes the world's leader for that business, with a presence in more that 30 countries in 3 continents. With nearly 3000 employees, the company serves hundred of billions of ads yearly, for a transaction total of 500 billions of dollars in 2016. 
			
			The main concern of Criteo is to bring back e-commerce costumers back to the retailer website so that they can concretize an ongoing order, a purchase new items. Criteo targets such users and proposes new product that fits their center of interests. Various techniques are employed to evaluate a given user's interests, its susceptibility of clicking on and ad and of converting to a sale on the advertiser website. 
			
			Criteo is the intermediary between two key actors: on the one hand, advertisers which signal the presence of quality users on their website, and who which to see them return. These involves a various member of e-commerce brands (La Redoute, Les 3 Suisses, ..), who trust Criteo to advertise for their product. On the other hand, publishers monetize their advertisement inventory. When a user loads a page on their website, they propose ad-placements on the page, usually through a bidding system. They involve large media website, or real-time bidding platforms like Google RTB. 
			
			\paragraph{} For obvious privacy and capacity reasons, Criteo can't store explicit information for every user that visit an advertiser website. Also, relationships between products (that could be advertised for) is extremely hard to engineer. Machine learning is a natural solution to these two problems, and is therefore at the very core of the technology proposed by Criteo. A wide variety of machine learning models is deployed at Criteo, and therefore a great extent of money and efforts are put in the training of these models. Also, pushed by the global enthusiasm around deep learning and the record breaking capacities it has shown, Criteo is pushing for its deployment in production, for many of its core technologies. 

		}

		\section{Motivations}
		{
			Numerical optimization is at the heart of most modern machine learning techniques, and often constitutes an important bottleneck when it comes to their deployment. Some of these techniques fall in mastered and well-known instances of numerical optimization problems (\emph{e.g} convex optimization, see \cite{nocedal2006numerical}) for which strong theoretical results have paved the way for the development of efficient new algorithms for many years. Recently, the advent of more complex models such a deep artificial neural networks led to the creation of several methods targeting high-dimensional, non-convex problems (the most famous ones being momentum \cite{nesterov1983method}, Adadelta \cite{zeiler2012adadelta} and Adam \cite{kingma2014adam}), now used as black-box algorithms by a majority of practitioners. Other attempts in this field use some additional problem-specific structure, like the work by \cite{martens2010deep} that leverages fast multiplication by the Hessian to yield better performing optimization policies, though computationally demanding. A common point to all these algorithms is that they leverage human-based understanding of loss surfaces, and usually require tuning hyper-parameters to achieve state-of-the-art performance. This tuning process can sometimes reveal mysterious behavior of the handled optimizers, making it reserved to human experts or the subject of a long and tedious search. Also, the process results in a static optimizer which excels at the specific task, but is likely to perform poorly on others. 
	
	\paragraph{}
	If the limitations of hand-designed algorithms come from poor human understanding of high-dimensional loss landscapes, it is natural to ask what machine learning can do for the design of optimization algorithms. Recently, \cite{andrychowicz2016learning} and \cite{li2016learning} both introduced two frameworks for learning optimization algorithms. While the former proposes to learn task-specific optimizers, the latter aims to produce task-independent optimization policies. While in the most general case this is bound to fail - as suggested by the No Free Lunch theorem for combinatorial optimization \cite{wolpert1997no} - we also believe that data driven techniques can be robust on a great variety of problems. \cite{li2016learning} manages to learn meta-learner that optimize a variety of neural network based losses (different datasets, different architecture), but fail to generalize to other problems. 
	
	\paragraph{} With this internship, we aimed at tackling the problem of meta-generalization and learn an optimizer that produces satisfying result on large classes of loss functions. It should display two main characteristics: an ability to navigate high-dimensional landscapes it has never seen during its meta-training, and also a lack of hyper-parameters which will enable it to be used truly as a black-box optimizer (and bypass the tedious hyper-parameter tuning phase).
		}
	}
	
	\section{Proposed approach and contributions}
	{
		Most optimization algorithms can be framed, for a given objective function $f$ and a current iterate $\theta_i$, as the problem of finding an appropriate update $\Delta \theta_i$. This update can for instance depend on past gradient information, rescaled gradient using curvature information or many other features. In a general manner, we can write $\Delta \theta_i = \phi( \theta_i, h(f,\theta_{i-1},\hdots,\theta_0),\xi)$ where $h(\cdot)$ denotes the set of features accumulated during the optimization procedure, and $\xi$ denotes the optimization hyper-parameters. 
		In our approach, we aim to bypass computing gradient and curvature information and learn the optimization features directly from data. This should allows us to obtain local state descriptors that can outperform classical features in terms of generalization on unseen loss functions and input data distributions. In this vein, we draw an analogy between learning an optimization algorithm and learning a navigation policy while having access to raw local observations of the landscape, which is also the inspiration for the name of our method, \emph{Rover Descent}. 
	Our algorithm contains three chained predictors that compute the angle of the move, the magnitude of the move (e.g. learning rate) and the resolution of the grid of the zeroth-order samples at the landing point. We train our navigation agent on hard \emph{prototypical 2D surfaces} in order to make sure we develop feature detectors and subsequent policies that will be able to lead to good decisions in difficult areas of the loss function. We pose both the learning rate and resolution predictor as reinforcement learning problems and introduce a reward-shaping formula that allows us to learn from functions with different magnitude and from multiple proto-families. In our experiments this was a crucial factor in being able to generalize on many different types of evaluation functions.
   	
	We show that this setup leads to very good convergence speeds both in two and higher dimensions, on evaluation functions that are not presented at training time. For a zeroth-order optimization algorithm, the convergence performance is surprisingly good, leading to results comparative to or better than the task-specific optimizer (\emph{e.g} the best one out of set of specifically tuned first and second order optimizers).
	
	 \paragraph{} In conclusion, we believe that our main contributions are the following: framing the learning to optimize problem as a navigation task,  proposing a zeroth-order information-based learning architecture, coupled with a proper training procedure on prototypical two-dimensional surfaces and a reward shaping formula and showing experimentally that it can match/outperform first and second order techniques on meta-generalization tasks.
	
	
	\section{Organization of the report}
	This report is organized as follows. We first give a few preliminary notions of deep learning and reinforcement learning in Chapter \ref{chap::prel} that will show useful in this report. We give a brief summary of past and recent related work in the field of learning to learn and learning to optimize, and position our approach with respect to existing work in Chapter \ref{chap::related}. We then develop in \ref{sec::twod} our approach in the two-dimensional case, before extending it to a higher dimensional setting in \ref{sec::highd}. We present experimental results in Chapter \ref{chap::results} that show the validity of our approach in a variety of setups. We finally develop potential ideas for future work in Chapter \ref{chap::future}. 

	}