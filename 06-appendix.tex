\chapter{Appendix}
{
	\section{Landscape generation in details}
	{
		\label{sec::landscape_detail}
		
		We generate random instances of five landscapes: saddles, valleys, plateaus, cliffs and quadratic bowls. The quadratic bowls are generated by sampling random matrices $A$ and vectors $b$ and aggregating them in a quadratic loss $\lVert Ax - b \rVert_2^2$. The other four landscapes are generated by creating random gaussian fields with Mahalanobis-norm covariance functions. More precisely, we carefully sample a given number of points $x$ which are attributed a random value $f(x)$. We then sample a covariance function $k(x,x') = \frac{1}{2} x^T S^{-1} x$ with $S$ being a randomly generated definite positive matrix, carefuly set to generate the targeted landscape.We therefore create a class of functions $\mathcal{F}_{train}$, from which we can sample random instances of the different targeted landscapes. We also add randomness inside each of these instances by not always taking the mean of the generated random fields, but by sampling inside the resulting distribution over space.
		
		We hereinafter describe precisely how we generated samples of each modality in $\mathcal{F}_{train}$ (except quadratic bowls). This procedure is extremely similar to the one that is followed when one makes inference with Gaussian Processes - see \cite{rasmussen2006gaussian}. It starts by carefully sampling a collections of points $\mathcal{X} = \{x_1,\hdots,x_k\}$ and their associated values $\mathcal{V} = \{v_1,\hdots,v_k\}$ to create a specificly targeted landscape. We then sample a positive definite scaling matrix $S$ for the normalized Gaussian covariance function: 
		\begin{equation}
			k(x,x') = \frac{1}{\vert 2\pi S\vert^{1/2}}\exp{\left(-\frac{1}{2}x^TS^{-1}x'\right)}
		\end{equation} 
		Once these steps are completed, and for $F(x) \triangleq \left(f(x),v_1,\hdots,v_k\right)$, we make a Gaussian hypothesis over the joint distribution:
		\begin{equation}
			 p(F) = \mathcal{N}\left( F\, \vert\, 0 , \begin{pmatrix} K \\ k(x) \end{pmatrix} \right)
		\end{equation} 
		where $K = (k(x_i,x_j))_{i,j}$ and $k(x) = (k(x,x_i))_i$. 
		We then can evaluate the conditional distribution $p(f(x) \, \vert \, v_1,\hdots,v_k)$ (which is also a Gaussian) and sample from it to create a noisy version of $f(x)$. We repeat this procedure whenever we need to access to the value of one of the loss in $\mathcal{F}_{train}$ at a point $x$. 
		
		\paragraph{} The following lists details how $\mathcal{X}$ and $\mathcal{V}$ were sampled for each modalities of $\mathcal{F}_{train}$.
		\begin{itemize}
			\item \textbf{Valleys}: We set $\mathcal{X} = \{ 0_{\mathbb{R}^2} \}$ and sample $v_1$ uniformly at random in $[-5,0]$. We sample one value $\lambda_1$ in a positive truncated Gaussian distribution centered at 10 with variance 2. We then multiply it by a ratio $\rho$ uniformly sampled at random inside the interval $[100,200]$ to obtain $\lambda_2 = \rho \lambda_1$. We sample uniformly at random $\phi$ in $[0,2\pi]$, and create $S = R_\phi^T \text{diag}(\lambda_1,\lambda_2) R_\phi$ where $R_\phi$ is the two-dimensional rotation matrix of angle $\phi$. By that mean, we are able to create valleys of different width and orientation.
			\item \textbf{Saddles} : we sample $4$ points $x_1,\hdots, x_4$ uniformly at random within each quarter of the square $[-1,1]^2$ and place them in $\mathcal{X}$. We assign a random value to each one (sampled from a Gaussian distribution) so that to opposite points have values of similar signs. We then sample $\lambda_1,\lambda_2$ in a truncated positive Gaussian distribution centered at $10$ and with variance $2$. We sample a random angle $\phi$ and compute $S = R_\phi^T \text{diag}(\lambda_1,\lambda_2) R_\phi$.
			\item \textbf{Plateau+cliffs}:  $\mathcal{X} = \{ 0_{\mathbb{R}^2} \}$  and generate a single value $v_1$ from a positive truncated Gaussian distribution centered in -5 with variance $2$. We then create a matrix $S$ in the same fashion as for the previously described landscape. 
		\end{itemize}

	}
	
	\section{Policy evaluation}
	{
		\label{sec::policy_viz} 
%		
%		\begin{figure}[h!]
%			\centering
%			\begin{subfigure}[b]{0.5\linewidth}
%			{
%				\centering
%				\includegraphics[width=0.8\textwidth]{policy_seq_quad}
%				\caption{Quadratic}
%				\label{fig::policy_seq_quadratic}
%			}
%			\end{subfigure}\hfill
%			\begin{subfigure}[b]{0.5\linewidth}
%			{
%				\centering
%				\includegraphics[width=0.8\textwidth]{policy_seq_valleys}
%				\caption{Valley}
%				\label{fig::policy_seq_valley}
%			}
%			\end{subfigure}\\
%			\begin{subfigure}[b]{0.5\linewidth}
%			{
%				\centering
%				\includegraphics[width=0.8\textwidth]{policy_seq_saddle}
%				\caption{Saddle}
%				\label{fig::policy_seq_saddle}
%			}
%			\end{subfigure}\hfill
%			\begin{subfigure}[b]{0.5\linewidth}
%			{
%				\centering
%				\includegraphics[width=0.8\textwidth]{policy_seq_cliff}
%				\caption{Plateau+cliff}
%				\label{fig::policy_seq_cliff}
%			}
%			\end{subfigure}
%			\caption{Step-size and resolution trajectories for different instances of $\mathcal{F}_{train}$}
%			\label{fig::policy_seq}
%		\end{figure}

%		
%		This curves could suggest that the policy has learnt good scheduling schemes for each of the modalities in $\mathcal{F}_{train}$. We show here that this is not the case, by showing that the policy is able to adapt to sharp changes of its initial state. In Figure \ref{fig::policy_seq_ad}, we sample this initial state far from the distribution that was used at training time (typically, a learning rate that is 100 times smaller that it usually is at training time, and a resolution 100 bigger), for a quadratic bowl. We can see that the schedule adapts to these changes, and therefore that mostly changes in the local landscape seen by the agent affects its decision (and not a fixed schedule learnt by the RNN). 
%		
%		\begin{figure}[h!]
%			\centering
%			\includegraphics[width=0.4\linewidth]{policy_seq_ad}
%			\caption{Step-size and resolution can recover from a poorly set initial state}
%			\label{fig::policy_seq_ad}
%		\end{figure}	

		In Figure \ref{fig::policy_seq_contour}, we show optimization runs on contour plots of modalities of $\mathcal{F}_{train}$, with a visualization of the sample grid. 
		\begin{figure}
			\centering
			\begin{subfigure}[b]{0.4\linewidth}
			{
				\centering
				\includegraphics[width=0.8\textwidth]{fplot_quad}
				\caption{Quadratic}
			}
			\end{subfigure}
			\begin{subfigure}[b]{0.4\linewidth}
			{
				\centering
				\includegraphics[width=0.8\textwidth]{fplot_valley}
				\caption{Valley}
			}
			\end{subfigure}\\
			\begin{subfigure}[b]{0.4\linewidth}
			{
				\centering
				\includegraphics[width=0.8\textwidth]{fplot_saddle}
				\caption{Saddle}
				\label{fig::policy_seq_saddle}
			}
			\end{subfigure}
			\begin{subfigure}[b]{0.4\linewidth}
			{
				\centering
				\includegraphics[width=0.8\textwidth]{fplot_cliff}
				\caption{Plateau+cliff}
				\label{fig::policy_seq_cliff}
			}
			\end{subfigure}
			\caption{Contour plot of optimization runs for policy vizualisation on $\mathcal{F}_{train}$}
			\label{fig::policy_seq_contour}
		\end{figure}
		
	}
	
	
	\section{Precisions on $\mathcal{F}_{test}$}
	{
		\label{sec::ftest}
		We provide here some contour plots for the functions used in the meta-test dataset. We also provide their mathematical expression, as well as the position of their global optimum and the position of the initial iterate we used in our experiment. 
		
		\subsection*{Rosenbrock's function}
		{
			Rosenbrock's function's analytical expression is:
			\begin{equation}
				f(x) = 100(x_1-x_0^2)^2 + (x_0-1)^2
			\end{equation}
			and its contour plot is shown in \ref{fig::rosenbrock}.
			
			\begin{figure}[h!]
				\begin{center}
					\includegraphics[width=0.3\linewidth]{rosenbrock_ls}
					\caption[Contour plot of the Rosenbrock's function]{Contour plot of the Rosenbrock's function. The black circle indicates the position of the optimum and the blue square the position of the initial iterate.}
					\label{fig::rosenbrock}
				\end{center}
			\end{figure}
			
		}
		
		\subsection*{Ackley's function}
		{
			Acley's function's analytical expression is:
			\begin{equation}
				f(x) = -20\exp{\left(-0.2\sqrt{\frac{1}{2}(x_1^2+x_2^2)}\right)} - \exp{\left(\frac{1}{2}\cos{2\pi x_1} + \frac{1}{2}\cos{2\pi x_2}\right)} + 20 + e^1
			\end{equation}
			and its contour plot is shown in \ref{fig::ackley}.
			
			\begin{figure}[h!]
				\begin{center}
					\includegraphics[width=0.3\linewidth]{ackley_ls}
					\caption[Contour plot of the Ackley's function]{Contour plot of the Ackley's function. The black circle indicates the position of the optimum and the blue square the position of the initial iterate.}
					\label{fig::ackley}
				\end{center}
			\end{figure}
		}
		
		\subsection*{Rastrigin's function}
		{
			Rastrigin's function's analytical expression is:
			\begin{equation}
				f(x) = 20 + \sum_{i=1}^2 (x_i^2 - 10\cos{(2\pi x_i)})
			\end{equation}
			and its contour plot is shown in \ref{fig::Rastrigin}.
			
			\begin{figure}[h!]
				\begin{center}
					\includegraphics[width=0.3\linewidth]{rastrigin_ls}
					\caption[Contour plot of the Rastrigin function]{Contour plot of the Rastrigin function. The black circle indicates the position of the optimum and the blue square the position of the initial iterate.}
					\label{fig::Rastrigin}
				\end{center}
			\end{figure}
		}
		
		\subsection*{Maccornick's function}
		{
			Maccornick's function's analytical expression is:
			\begin{equation}
				f(x) = \sin{(x_1+x_2)} + (x_1-x_2)^2 -1.5x_1 + 2.5x_2 +1 
			\end{equation}
			and its contour plot is shown in \ref{fig::Maccornick}.
			
			\begin{figure}[h!]
				\begin{center}
					\includegraphics[width=0.3\linewidth]{mc_cornick_ls}
					\caption[Contour plot of the Rastrigin function]{Contour plot of the Maccornick function. The black circle indicates the position of the optimum and the blue square the position of the initial iterate.}
					\label{fig::Maccornick}
				\end{center}
			\end{figure}
		}
		
		\subsection*{Styblinski's function}
		{
			Styblinski's function's analytical expression is:
			\begin{equation}
				f(x) = \frac{1}{2} \sum_{i=1}^2 (x_i^4 - 16x_i^2+5x_i)
			\end{equation}
			and its contour plot is shown in \ref{fig::Styblinski}.
			
			\begin{figure}[h!]
				\begin{center}
					\includegraphics[width=0.3\linewidth]{styblinksi_ls}
					\caption[Contour plot of the Styblinski function]{Contour plot of the Styblinski function. The black circle indicates the position of the optimum and the blue square the position of the initial iterate.}
					\label{fig::Styblinski}
				\end{center}
			\end{figure}
		}
		
		\subsection*{Beale's function}
		{
			Beale's function's analytical expression is:
			\begin{equation}
				f(x) = (1.5-x_1+x_1x_2)^2 + (2.25-x_1+x_1x_2^2)^2 + (2.625-x_1+x_1x_2^3)^2
			\end{equation}
			and its contour plot is shown in \ref{fig::Beale}.
			
			\begin{figure}[h!]
				\begin{center}
					\includegraphics[width=0.3\linewidth]{beale_ls}
					\caption[Contour plot of the Beale function]{Contour plot of the Beale function. The black circle indicates the position of the optimum and the blue square the position of the initial iterate.}
					\label{fig::Beale}
				\end{center}
			\end{figure}
		}
	}
}